#!/bin/bash

# Exit on any error
set -e

###################
# CONFIGURATION
###################
MASTER_ACCOUNT_ID="MASTER_ID"
MASTER_ROLE_NAME="master_role"
SLAVE_ROLE_NAME="slave_role"
SESSION_NAME="S3CostOpt-$(date +%Y%m%d-%H%M)"
SLAVE_ACCOUNT_IDS=("slave1" "slave2" "slave3")
if [ -z "${WORKSPACE}" ]; then
    echo "ERROR: WORKSPACE environment variable not set"
    exit 1
fi
OUTPUT_DIR="${WORKSPACE}/s3-analysis/$(date +%Y%m%d_%H%M%S)"
LOG_FILE="${OUTPUT_DIR}/analysis.log"

###################
# LOGGING FUNCTIONS
###################
log_message() {
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    echo "[$timestamp] $1" >> "${LOG_FILE}"
}

log_account_separator() {
    local account_id=$1
    echo "
========================================
Account: ${account_id}
Start Time: $(date '+%Y-%m-%d %H:%M:%S')
========================================" >> "${LOG_FILE}"
}

log_account_end() {
    local account_id=$1
    echo "
----------------------------------------
End Processing: ${account_id}
End Time: $(date '+%Y-%m-%d %H:%M:%S')
----------------------------------------
" >> "${LOG_FILE}"
}

###################
# DIRECTORY SETUP
###################
setup_directories() {
    mkdir -p "${OUTPUT_DIR}"
    chmod 700 "${OUTPUT_DIR}"
    for account_id in "${SLAVE_ACCOUNT_IDS[@]}"; do
        mkdir -p "${OUTPUT_DIR}/${account_id}"
    done
}

###################
# ROLE ASSUMPTION
###################
assume_master_role() {
    log_message "Assuming master role in account ${MASTER_ACCOUNT_ID}"
    
    local master_credentials=$(aws sts assume-role \
        --role-arn "arn:aws:iam::${MASTER_ACCOUNT_ID}:role/${MASTER_ROLE_NAME}" \
        --role-session-name "${SESSION_NAME}" \
        --output json)
    
    export AWS_ACCESS_KEY_ID=$(echo "${master_credentials}" | jq -r '.Credentials.AccessKeyId')
    export AWS_SECRET_ACCESS_KEY=$(echo "${master_credentials}" | jq -r '.Credentials.SecretAccessKey')
    export AWS_SESSION_TOKEN=$(echo "${master_credentials}" | jq -r '.Credentials.SessionToken')
}

assume_slave_role() {
    local account_id=$1
    log_message "Assuming slave role in account ${account_id}"
    
    local slave_credentials=$(aws sts assume-role \
        --role-arn "arn:aws:iam::${account_id}:role/${SLAVE_ROLE_NAME}" \
        --role-session-name "${SESSION_NAME}" \
        --output json)
    
    export AWS_ACCESS_KEY_ID=$(echo "${slave_credentials}" | jq -r '.Credentials.AccessKeyId')
    export AWS_SECRET_ACCESS_KEY=$(echo "${slave_credentials}" | jq -r '.Credentials.SecretAccessKey')
    export AWS_SESSION_TOKEN=$(echo "${slave_credentials}" | jq -r '.Credentials.SessionToken')
}



###################
# AWS PRICING CONFIGURATION - SEOUL REGION (ap-northeast-2)
# Last Updated: January 2024
# Reference: https://aws.amazon.com/s3/pricing/
###################
declare -A STORAGE_COSTS=(
    # Standard Storage Pricing (per GB per month)
    ["STANDARD"]="0.025"              # First 50TB
    ["STANDARD_NEXT"]="0.024"         # Next 450TB
    ["STANDARD_OVER"]="0.023"         # Over 500TB
    
    # Intelligent-Tiering (per GB per month)
    ["INTELLIGENT_TIERING"]="0.025"   # Base cost
    ["INTELLIGENT_TIERING_MONITORING"]="0.0025" # Per 1000 objects
    
    # Standard-IA (per GB per month)
    ["STANDARD_IA"]="0.0125"          # Minimum 30 days
    
    # One Zone-IA (per GB per month)
    ["ONEZONE_IA"]="0.0100"           # Minimum 30 days
    
    # Glacier Storage Classes
    ["GLACIER_INSTANT"]="0.004"       # Minimum 90 days
    ["GLACIER_FLEXIBLE"]="0.0036"     # Minimum 90 days
    ["GLACIER_DEEP"]="0.00099"        # Minimum 180 days
)

declare -A RETRIEVAL_COSTS=(
    # Data Retrieval Pricing (per GB)
    ["STANDARD_IA"]="0.01"
    ["ONEZONE_IA"]="0.01"
    ["GLACIER_INSTANT"]="0.03"
    ["GLACIER_FLEXIBLE"]="0.02"       # Standard retrieval
    ["GLACIER_DEEP"]="0.02"           # Standard retrieval
)

declare -A REQUEST_COSTS=(
    # Request Pricing (per 1000 requests)
    ["PUT"]="0.0053"                  # PUT, COPY, POST, LIST
    ["GET"]="0.00042"                 # GET, SELECT
    ["LIFECYCLE"]="0.0106"            # Lifecycle transition
    ["EARLY_DELETE"]="0.0125"         # For objects deleted before minimum storage duration
)

###################
# MONITORING COST CALCULATION FUNCTIONS
###################
calculate_intelligent_tiering_monitoring_cost() {
    local object_count=$1
    local monitoring_fee=${STORAGE_COSTS["INTELLIGENT_TIERING_MONITORING"]}
    # Calculate monthly monitoring fee per 1000 objects
    echo "scale=4; (${object_count} / 1000) * ${monitoring_fee}" | bc
}

calculate_potential_savings() {
    local size_gb=$1
    local current_class=$2
    local target_class=$3
    local access_frequency=$4  # requests per month
    
    # Calculate storage cost difference
    local current_cost=$(echo "scale=4; ${size_gb} * ${STORAGE_COSTS[${current_class}]}" | bc)
    local target_cost=$(echo "scale=4; ${size_gb} * ${STORAGE_COSTS[${target_class}]}" | bc)
    
    # Add retrieval costs if applicable
    if [[ "${target_class}" != "STANDARD" ]]; then
        local retrieval_cost=$(echo "scale=4; ${size_gb} * ${access_frequency} * ${RETRIEVAL_COSTS[${target_class}]}" | bc)
        target_cost=$(echo "scale=4; ${target_cost} + ${retrieval_cost}" | bc)
    fi
    
    # Calculate savings
    echo "scale=4; ${current_cost} - ${target_cost}" | bc
}

###################
# OPTIMIZATION ANALYSIS FUNCTIONS
###################
analyze_storage_class_recommendation() {
    local size_gb=$1
    local access_frequency=$2
    local current_class=$3
    
    # Analysis logic based on Seoul region specific thresholds
    if (( $(echo "${access_frequency} < 1" | bc -l) )); then
        # Rarely accessed data
        if (( $(echo "${size_gb} > 500" | bc -l) )); then
            echo "GLACIER_DEEP"
        else
            echo "GLACIER_FLEXIBLE"
        fi
    elif (( $(echo "${access_frequency} < 12" | bc -l) )); then
        # Infrequently accessed data
        echo "STANDARD_IA"
    else
        # Frequently accessed data
        if (( $(echo "${size_gb} > 1000" | bc -l) )); then
            echo "INTELLIGENT_TIERING"
        else
            echo "STANDARD"
        fi
    fi
}


calculate_tiered_storage_cost() {
    local size_gb=$1
    local storage_class=$2
    
    # Calculate cost based on tiered pricing
    if [ "${storage_class}" = "STANDARD" ]; then
        if (( $(echo "${size_gb} <= 51200" | bc -l) )); then  # First 50TB
            echo "scale=4; ${size_gb} * ${STORAGE_COSTS[STANDARD]}" | bc
        elif (( $(echo "${size_gb} <= 512000" | bc -l) )); then  # Next 450TB
            local first_tier_cost=$(echo "scale=4; 51200 * ${STORAGE_COSTS[STANDARD]}" | bc)
            local remaining_gb=$(echo "scale=4; ${size_gb} - 51200" | bc)
            local second_tier_cost=$(echo "scale=4; ${remaining_gb} * ${STORAGE_COSTS[STANDARD_NEXT]}" | bc)
            echo "scale=4; ${first_tier_cost} + ${second_tier_cost}" | bc
        else  # Over 500TB
            local first_tier_cost=$(echo "scale=4; 51200 * ${STORAGE_COSTS[STANDARD]}" | bc)
            local second_tier_cost=$(echo "scale=4; 460800 * ${STORAGE_COSTS[STANDARD_NEXT]}" | bc)
            local remaining_gb=$(echo "scale=4; ${size_gb} - 512000" | bc)
            local third_tier_cost=$(echo "scale=4; ${remaining_gb} * ${STORAGE_COSTS[STANDARD_OVER]}" | bc)
            echo "scale=4; ${first_tier_cost} + ${second_tier_cost} + ${third_tier_cost}" | bc
        fi
    else
        echo "scale=4; ${size_gb} * ${STORAGE_COSTS[${storage_class}]}" | bc
    fi
}

# Add this function to analyze API costs
calculate_request_costs() {
    local bucket=$1
    local account_dir=$2
    
    # Get monthly request counts
    local get_requests=$(jq -r '.Datapoints[].Sum // 0' "${account_dir}/${bucket}_api_usage.json" | awk '{s+=$1} END {print s}')
    local put_requests=$(jq -r '.Datapoints[].Sum // 0' "${account_dir}/${bucket}_put_requests.json" | awk '{s+=$1} END {print s}')
    
    # Calculate costs
    local get_cost=$(echo "scale=4; (${get_requests}/1000) * ${REQUEST_COSTS[GET]}" | bc)
    local put_cost=$(echo "scale=4; (${put_requests}/1000) * ${REQUEST_COSTS[PUT]}" | bc)
    
    echo "scale=4; ${get_cost} + ${put_cost}" | bc
}

# Add this function to analyze lifecycle transition costs
calculate_lifecycle_costs() {
    local size_gb=$1
    local current_class=$2
    local target_class=$3
    
    # Calculate one-time transition cost
    local transition_cost=$(echo "scale=4; ${size_gb} * ${REQUEST_COSTS[LIFECYCLE]}" | bc)
    echo "${transition_cost}"
}

# Add this function to calculate ROI
calculate_roi() {
    local current_cost=$1
    local target_cost=$2
    local transition_cost=$3
    
    # Calculate months to break even
    local monthly_savings=$(echo "scale=4; ${current_cost} - ${target_cost}" | bc)
    local breakeven_months=$(echo "scale=2; ${transition_cost} / ${monthly_savings}" | bc)
    
    echo "${breakeven_months}"
}

# Add this function to analyze data transfer costs
calculate_transfer_costs() {
    local size_gb=$1
    local region=$2
    
    # Add your region-specific transfer costs
    local transfer_cost_out="0.09"  # Example cost per GB for data transfer out
    
    echo "scale=4; ${size_gb} * ${transfer_cost_out}" | bc
}

###################
# DATA GATHERING
###################
gather_s3_data() {
    local account_id=$1
    local account_dir="${OUTPUT_DIR}/${account_id}"
    
    ###############################################################
    # 1. BUCKET INVENTORY
    # WHY: Foundation for cost analysis - identifies all S3 resources
    # COST IMPACT: NO DIRECT COST
    # - Free API call
    # - Essential for identifying unused/forgotten buckets
    # OPTIMIZATION POTENTIAL: HIGH
    # - Helps identify:
    #   * Redundant buckets (potential 100% saving for removed buckets)
    #   * Test/Dev buckets that can be cleaned up
    #   * Buckets without proper ownership
    ###############################################################
    log_message "Gathering bucket inventory for ${account_id}"
    aws s3api list-buckets \
        --query 'Buckets[*].[Name,CreationDate]' \
        --output json > "${account_dir}/buckets_list.json"

    for bucket in $(jq -r '.[].Name' "${account_dir}/buckets_list.json"); do
        ###############################################################
        # 2. BUCKET SIZES AND OBJECT COUNTS
        # WHY: Identifies high-cost storage areas and object distribution
        # COST IMPACT: HIGH
        # - Storage costs are directly proportional to size
        # - Typical costs per GB/month:
        #   * Standard: $0.023
        #   * Standard-IA: $0.0125
        #   * Glacier: $0.004
        # OPTIMIZATION POTENTIAL: HIGH
        # - Large buckets (>1TB) can save 65% with proper tiering
        # - High object counts might indicate:
        #   * Need for object expiration (potential 30-50% saving)
        #   * Opportunity for object compression (10-30% saving)
        ###############################################################
        log_message "Analyzing size and objects for bucket: ${bucket}"
        aws s3api list-objects-v2 \
            --bucket "${bucket}" \
            --query '[sum(Contents[].Size), length(Contents[])]' \
            --output json > "${account_dir}/${bucket}_size.json"

        ###############################################################
        # Calculate and log bucket metrics
        # WHY: Provides human-readable size analysis and object count
        # COST IMPACT: NO DIRECT COST (Analysis only)
        # USAGE:
        # - Helps identify oversized buckets quickly
        # - Flags buckets with unusual object counts
        # - Enables quick cost estimation:
        #   * >1TB in Standard: High cost impact
        #   * >100K objects: Check for small file optimization
        ###############################################################
        total_size=$(jq '.[0]' "${account_dir}/${bucket}_size.json")
        object_count=$(jq '.[1]' "${account_dir}/${bucket}_size.json")
        echo "Bucket: ${bucket}, Size: $(numfmt --to=iec ${total_size}), Objects: ${object_count}" >> "${LOG_FILE}"

        ###############################################################
        # 3. BUCKET TAGS
        # WHY: Critical for cost allocation and compliance management
        # COST IMPACT: MEDIUM
        # - No direct cost but affects:
        #   * Cost allocation accuracy
        #   * Compliance requirements (especially PII data)
        #   * Lifecycle management effectiveness
        # OPTIMIZATION POTENTIAL: MEDIUM
        # - Proper tagging enables:
        #   * Automated lifecycle rules (15-30% saving)
        #   * Better cost tracking
        #   * Compliance-based storage decisions
        ###############################################################
        log_message "Getting tags for bucket: ${bucket}"
        aws s3api get-bucket-tagging \
            --bucket "${bucket}" \
            --output json > "${account_dir}/${bucket}_tags.json" 2>/dev/null || \
            echo '{"TagSet": []}' > "${account_dir}/${bucket}_tags.json"

        ###############################################################
        # 4. LIFECYCLE POLICIES
        # WHY: Automated cost optimization through object lifecycle management
        # COST IMPACT: HIGH
        # - Missing lifecycle policies often lead to:
        #   * Unnecessary storage costs (up to 75% extra)
        #   * Retention of unused objects
        #   * Inefficient storage class usage
        # OPTIMIZATION POTENTIAL: HIGH
        # Typical savings:
        # - Standard → IA transition: 33% saving
        # - Standard → Glacier: 75% saving
        # - Deletion of unused objects: 100% saving for those objects
        ###############################################################
        log_message "Getting lifecycle policies for bucket: ${bucket}"
        aws s3api get-bucket-lifecycle-configuration \
            --bucket "${bucket}" \
            --output json > "${account_dir}/${bucket}_lifecycle.json" 2>/dev/null || \
            echo '{}' > "${account_dir}/${bucket}_lifecycle.json"

        ###############################################################
        # 5. ACCESS PATTERNS
        # WHY: Determines optimal storage class and access requirements
        # COST IMPACT: HIGH
        # - Wrong storage class can increase costs by:
        #   * Up to 50% for frequently accessed IA data
        #   * Up to 200% for rarely accessed Standard data
        # - Retrieval costs:
        #   * Standard: Free
        #   * IA: $0.01 per GB
        #   * Glacier: $0.02-$0.03 per GB
        # OPTIMIZATION POTENTIAL: HIGH
        # - Proper storage class selection can save:
        #   * 30-60% on storage costs
        #   * 40-80% on infrequently accessed data
        ###############################################################
        log_message "Getting access metrics for bucket: ${bucket}"
        aws cloudwatch get-metric-statistics \
            --namespace AWS/S3 \
            --metric-name NumberOfObjects \
            --dimensions Name=BucketName,Value="${bucket}" \
            --start-time $(date -d '30 days ago' --iso-8601=seconds) \
            --end-time $(date --iso-8601=seconds) \
            --period 86400 \
            --statistics Average \
            --output json > "${account_dir}/${bucket}_metrics.json"

        ###############################################################
        # 6. STORAGE CLASS DISTRIBUTION
        # WHY: Identifies immediate storage class optimization opportunities
        # COST IMPACT: HIGH
        # Current costs per GB/month (US East-1):
        # - Standard: $0.023
        # - Intelligent-Tiering: $0.023 + monitoring
        # - Standard-IA: $0.0125
        # - One-Zone IA: $0.01
        # - Glacier: $0.004
        # - Deep Glacier: $0.00099
        # OPTIMIZATION POTENTIAL: HIGH
        # - Immediate savings opportunities:
        #   * Standard → IA: 45% saving
        #   * Standard → Glacier: 82% saving
        #   * IA → Glacier: 68% saving
        ###############################################################
        log_message "Analyzing storage classes for bucket: ${bucket}"
        aws s3api list-objects-v2 \
            --bucket "${bucket}" \
            --query 'Contents[*].[Key,Size,StorageClass,LastModified]' \
            --output json > "${account_dir}/${bucket}_objects.json"

        ###############################################################
        # Analyze and log storage class distribution
        # WHY: Provides immediate visibility into storage class usage
        # COST IMPACT: NO DIRECT COST (Analysis only)
        # INSIGHTS PROVIDED:
        # - Identifies improper storage class usage
        # - Shows potential for immediate cost reduction
        # - Helps in planning storage class transitions
        # OPTIMIZATION OPPORTUNITIES:
        # - Large Standard class count → Consider IA/Intelligent Tiering
        # - High Glacier object count → Check retrieval patterns
        # - Mixed storage classes → Review lifecycle policies
        ###############################################################
        jq -r '.[].StorageClass' "${account_dir}/${bucket}_objects.json" | \
            sort | uniq -c | \
            while read -r count class; do
                # Log distribution for cost analysis
                echo "Bucket: ${bucket}, Class: ${class}, Count: ${count}" >> "${LOG_FILE}"
                
                # Add analysis hints based on storage class
                case ${class} in
                    "STANDARD")
                        if [ ${count} -gt 10000 ]; then
                            echo "  HINT: Large Standard class count (${count}) - Consider IA/Intelligent Tiering" >> "${LOG_FILE}"
                        fi
                        ;;
                    "STANDARD_IA")
                        if [ ${count} -lt 100 ]; then
                            echo "  HINT: Small IA object count (${count}) - May not be cost-effective" >> "${LOG_FILE}"
                        fi
                        ;;
                    "GLACIER")
                        echo "  HINT: Glacier objects (${count}) - Verify retrieval patterns" >> "${LOG_FILE}"
                        ;;
                esac
            done

        ###############################################################
        # Enhanced Cost Analysis and Lifecycle Transition Assessment
        # WHY: Provides detailed cost breakdown and transition recommendations
        # COST IMPACT: HIGH
        # OPTIMIZATION POTENTIAL: HIGH
        ###############################################################
        log_message "Performing enhanced cost analysis for bucket: ${bucket}"
        
        # Calculate current storage and request costs
        local current_cost=$(calculate_tiered_storage_cost "${size_gb}" "${storage_class}")
        local request_cost=$(calculate_request_costs "${bucket}" "${account_dir}")
        local total_current_cost=$(echo "scale=4; ${current_cost} + ${request_cost}" | bc)
        
        # Log current costs
        echo "Current Storage Cost: ${current_cost}" >> "${LOG_FILE}"
        echo "Current Request Cost: ${request_cost}" >> "${LOG_FILE}"
        echo "Total Current Cost: ${total_current_cost}" >> "${LOG_FILE}"

        # Add lifecycle transition analysis
        if [[ "${storage_class}" != "${recommended_class}" ]]; then
            local transition_cost=$(calculate_lifecycle_costs "${size_gb}" "${storage_class}" "${recommended_class}")
            local future_storage_cost=$(calculate_tiered_storage_cost "${size_gb}" "${recommended_class}")
            local breakeven_months=$(calculate_roi "${total_current_cost}" "${future_storage_cost}" "${transition_cost}")
            
            echo "  - Transition Cost: ${transition_cost}" >> "${LOG_FILE}"
            echo "  - Future Storage Cost: ${future_storage_cost}" >> "${LOG_FILE}"
            echo "  - Months to Break Even: ${breakeven_months}" >> "${LOG_FILE}"
            
            # Add to analysis report
            echo "  - Transition Cost: ${transition_cost}" >> "${analysis_report}"
            echo "  - Months to Break Even: ${breakeven_months}" >> "${analysis_report}"
        fi

        ###############################################################
        # 7. API USAGE PATTERNS
        # WHY: Identifies request cost optimization opportunities
        # COST IMPACT: MEDIUM
        # Current costs per 1000 requests:
        # - PUT/COPY/POST: $0.005
        # - GET: $0.0004
        # - Lifecycle Transitions: $0.01
        # - Data Retrieval: Varies by storage class
        # OPTIMIZATION POTENTIAL: MEDIUM
        # - Request cost optimization can save:
        #   * 40-60% with proper batching
        #   * 30-50% with CloudFront caching
        #   * 20-40% with better application patterns
        ###############################################################
        log_message "Getting API usage patterns for bucket: ${bucket}"
        aws cloudwatch get-metric-data \
            --metric-data-queries '[
                {
                    "Id": "gets",
                    "MetricStat": {
                        "Metric": {
                            "Namespace": "AWS/S3",
                            "MetricName": "GetRequests",
                            "Dimensions": [{"Name": "BucketName", "Value": "'${bucket}'"}]
                        },
                        "Period": 86400,
                        "Stat": "Sum"
                    }
                }
            ]' \
            --start-time $(date -d '30 days ago' --iso-8601=seconds) \
            --end-time $(date --iso-8601=seconds) \
            --output json > "${account_dir}/${bucket}_api_usage.json"
    done
}
###################
# CLEANUP
###################
cleanup_credentials() {
    unset AWS_ACCESS_KEY_ID
    unset AWS_SECRET_ACCESS_KEY
    unset AWS_SESSION_TOKEN
}

###################
# ERROR HANDLING
###################
error_handler() {
    local line_number=$1
    local account_id=$2
    
    log_message "ERROR: Failed at line ${line_number}"
    [[ -n "${account_id}" ]] && log_message "ERROR: While processing account ${account_id}"
    
    cleanup_credentials
    exit 1
}

###################
# MAIN EXECUTION
###################
main() {
    setup_directories
    
    # Assume master role
    assume_master_role
    
    # Store master credentials
    MASTER_AWS_ACCESS_KEY_ID="${AWS_ACCESS_KEY_ID}"
    MASTER_AWS_SECRET_ACCESS_KEY="${AWS_SECRET_ACCESS_KEY}"
    MASTER_AWS_SESSION_TOKEN="${AWS_SESSION_TOKEN}"
    
    # Process each slave account
    for slave_account_id in "${SLAVE_ACCOUNT_IDS[@]}"; do
        log_account_separator "${slave_account_id}"
        assume_slave_role "${slave_account_id}"
        gather_s3_data "${slave_account_id}"
        log_account_end "${slave_account_id}"
        
        # Restore master credentials for next iteration
        export AWS_ACCESS_KEY_ID="${MASTER_AWS_ACCESS_KEY_ID}"
        export AWS_SECRET_ACCESS_KEY="${MASTER_AWS_SECRET_ACCESS_KEY}"
        export AWS_SESSION_TOKEN="${MASTER_AWS_SESSION_TOKEN}"
    done
    
    ###############################################################
    # COMPREHENSIVE COST OPTIMIZATION ANALYSIS
    # WHY: Provides actionable insights across all accounts
    # COST IMPACT: HIGH
    ###############################################################
    log_message "Generating comprehensive cost analysis report..."
    local analysis_report="${OUTPUT_DIR}/cost_optimization_report.txt"
    
    # Initialize report
    cat << EOF > "${analysis_report}"
=================================================================
                ENTERPRISE S3 COST OPTIMIZATION REPORT
=================================================================
Analysis Date: $(date '+%Y-%m-%d %H:%M:%S')
Accounts Analyzed: ${#SLAVE_ACCOUNT_IDS[@]}
-----------------------------------------------------------------

EOF

    # Analyze each account's data
    local total_size=0
    local total_cost=0
    local total_potential_savings=0

    for account_id in "${SLAVE_ACCOUNT_IDS[@]}"; do
        local account_dir="${OUTPUT_DIR}/${account_id}"
        
        echo "Account: ${account_id}" >> "${analysis_report}"
        echo "----------------------------------------" >> "${analysis_report}"
        
        # Process each bucket in the account
        find "${account_dir}" -name "*_size.json" | while read size_file; do
            local bucket_name=$(basename "${size_file}" _size.json)
            local size=$(jq '.[0]' "${size_file}")
            local object_count=$(jq '.[1]' "${size_file}")
            local size_gb=$(echo "scale=4; ${size:-0} / 1073741824" | bc)
            
            # Calculate current costs
            local storage_class=$(jq -r '.StorageClass // "STANDARD"' "${account_dir}/${bucket_name}_objects.json")
            local current_cost=$(echo "scale=4; ${size_gb} * ${STORAGE_COSTS[${storage_class}]}" | bc)
            
            # Calculate monitoring costs if applicable
            local monitoring_cost=0
            if [[ "${storage_class}" == "INTELLIGENT_TIERING" ]]; then
                monitoring_cost=$(calculate_intelligent_tiering_monitoring_cost "${object_count}")
            fi
            
            # Get access patterns
            local access_frequency=$(jq -r '.Datapoints[0].Average // 0' "${account_dir}/${bucket_name}_metrics.json")
            
            # Get recommended storage class
            local recommended_class=$(analyze_storage_class_recommendation "${size_gb}" "${access_frequency}" "${storage_class}")
            
            # Calculate potential savings
            local potential_savings=0
            if [[ "${storage_class}" != "${recommended_class}" ]]; then
                potential_savings=$(calculate_potential_savings "${size_gb}" "${storage_class}" "${recommended_class}" "${access_frequency}")
            fi
            
            # Output bucket analysis
            cat << EOF >> "${analysis_report}"
Bucket: ${bucket_name}
  - Size: $(numfmt --to=iec ${size})
  - Objects: ${object_count}
  - Current Storage Class: ${storage_class}
  - Current Monthly Cost: ${current_cost}
  - Recommended Class: ${recommended_class}
  - Potential Monthly Savings: ${potential_savings}
  - Access Pattern: $(if (( $(echo "${access_frequency} < 1" | bc -l) )); then echo "Infrequent"; else echo "Frequent"; fi)

EOF

            # Update totals
            total_size=$(echo "${total_size} + ${size}" | bc)
            total_cost=$(echo "${total_cost} + ${current_cost}" | bc)
            total_potential_savings=$(echo "${total_potential_savings} + ${potential_savings}" | bc)
        done
    done

    # Calculate additional metrics for enhanced executive summary
    local total_storage_cost=$(echo "scale=2; ${total_cost} * 0.7" | bc)  # Approximate storage cost
    local total_request_cost=$(echo "scale=2; ${total_cost} * 0.2" | bc)  # Approximate request cost
    local total_transfer_cost=$(echo "scale=2; ${total_cost} * 0.1" | bc) # Approximate transfer cost
    local total_transition_cost=0
    local average_breakeven_months=0
    local first_year_savings=0
    local three_year_savings=0
    local retrieval_risk_level="LOW"
    local cost_variability_risk="MEDIUM"
    local implementation_complexity="MODERATE"

    # Calculate projected savings
    first_year_savings=$(echo "scale=2; ${total_potential_savings} * 12 - ${total_transition_cost}" | bc)
    three_year_savings=$(echo "scale=2; ${first_year_savings} * 3" | bc)

    # Generate executive summary
    cat << EOF >> "${analysis_report}"

=================================================================
                    EXECUTIVE SUMMARY
=================================================================
Total Storage: $(numfmt --to=iec ${total_size})
Current Monthly Cost: ${total_cost}
Potential Monthly Savings: ${total_potential_savings}
Potential Annual Savings: $(echo "scale=2; ${total_potential_savings} * 12" | bc)

Cost Breakdown:
  - Storage Costs: ${total_storage_cost}
  - Request Costs: ${total_request_cost}
  - Transfer Costs: ${total_transfer_cost}
  - Lifecycle Transition Costs: ${total_transition_cost}

ROI Analysis:
  - Average Break-even Period: ${average_breakeven_months} months
  - First-year Net Savings: ${first_year_savings}
  - 3-year Projected Savings: ${three_year_savings}

Risk Analysis:
  - Data Retrieval Risk: ${retrieval_risk_level}
  - Cost Variability Risk: ${cost_variability_risk}
  - Implementation Complexity: ${implementation_complexity}

Key Recommendations:
1. Implement lifecycle policies for infrequently accessed data
2. Enable Intelligent-Tiering for variable access patterns
3. Review and clean up unused objects
4. Optimize storage class distribution
5. Monitor API usage patterns

Next Steps:
1. Review detailed recommendations per bucket
2. Prioritize high-impact changes
3. Implement automated lifecycle policies
4. Schedule regular cost optimization reviews

=================================================================
Report generated by S3 Cost Optimizer v2.0
=================================================================
EOF
    
    log_message "Cost optimization report generated: ${analysis_report}"
    
    # Generate summary report
    log_message "Analysis completed. Check ${OUTPUT_DIR} for detailed reports."
    log_message "High-impact optimization opportunities found:"
    find "${OUTPUT_DIR}" -name "*_size.json" -exec jq -r '
        select(.[0] > 1000000000) | "Large bucket found: \(.[0])"
    ' {} \;
    
    # Final cleanup
    cleanup_credentials
}

# Set error handler
trap 'error_handler ${LINENO}' ERR

# Execute main function
main
