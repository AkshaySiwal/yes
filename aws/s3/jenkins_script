#!/bin/bash

# Exit on any error
set -e

###################
# CONFIGURATION
###################
MASTER_ACCOUNT_ID="MASTER_ID"
MASTER_ROLE_NAME="master_role"
SLAVE_ROLE_NAME="slave_role"
SESSION_NAME="S3CostOpt-$(date +%Y%m%d-%H%M)"
SLAVE_ACCOUNT_IDS=("slave1" "slave2" "slave3")

# Workspace validation
if [ -z "${WORKSPACE}" ]; then
    echo "ERROR: WORKSPACE environment variable not set"
    exit 1
fi
OUTPUT_DIR="${WORKSPACE}/s3-analysis/$(date +%Y%m%d_%H%M%S)"
LOG_FILE="${OUTPUT_DIR}/analysis.log"

###################
# LOGGING FUNCTIONS
###################
log_message() {
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    echo "[$timestamp] \$1" >> "${LOG_FILE}"
}

log_account_separator() {
    local account_id=\$1
    echo "
========================================
Account: ${account_id}
Start Time: $(date '+%Y-%m-%d %H:%M:%S')
========================================" >> "${LOG_FILE}"
}

log_account_end() {
    local account_id=\$1
    echo "
----------------------------------------
End Processing: ${account_id}
End Time: $(date '+%Y-%m-%d %H:%M:%S')
----------------------------------------
" >> "${LOG_FILE}"
}

###################
# DIRECTORY SETUP
###################
setup_directories() {
    mkdir -p "${OUTPUT_DIR}"
    chmod 700 "${OUTPUT_DIR}"
    for account_id in "${SLAVE_ACCOUNT_IDS[@]}"; do
        mkdir -p "${OUTPUT_DIR}/${account_id}"
    done
}

###################
# ROLE ASSUMPTION
###################
assume_master_role() {
    log_message "Assuming master role in account ${MASTER_ACCOUNT_ID}"
    
    local master_credentials
    master_credentials=$(aws sts assume-role \
        --role-arn "arn:aws:iam::${MASTER_ACCOUNT_ID}:role/${MASTER_ROLE_NAME}" \
        --role-session-name "${SESSION_NAME}" \
        --output json)
    
    export AWS_ACCESS_KEY_ID=$(echo "${master_credentials}" | jq -r '.Credentials.AccessKeyId')
    export AWS_SECRET_ACCESS_KEY=$(echo "${master_credentials}" | jq -r '.Credentials.SecretAccessKey')
    export AWS_SESSION_TOKEN=$(echo "${master_credentials}" | jq -r '.Credentials.SessionToken')
}

assume_slave_role() {
    local account_id=\$1
    log_message "Assuming slave role in account ${account_id}"
    
    local slave_credentials
    slave_credentials=$(aws sts assume-role \
        --role-arn "arn:aws:iam::${account_id}:role/${SLAVE_ROLE_NAME}" \
        --role-session-name "${SESSION_NAME}" \
        --output json)
    
    export AWS_ACCESS_KEY_ID=$(echo "${slave_credentials}" | jq -r '.Credentials.AccessKeyId')
    export AWS_SECRET_ACCESS_KEY=$(echo "${slave_credentials}" | jq -r '.Credentials.SecretAccessKey')
    export AWS_SESSION_TOKEN=$(echo "${slave_credentials}" | jq -r '.Credentials.SessionToken')
}

###################
# AWS PRICING CONFIGURATION - SEOUL REGION (ap-northeast-2)
# Last Updated: January 2024
# Reference: https://aws.amazon.com/s3/pricing/
###################
declare -A STORAGE_COSTS=(
    # Standard Storage Pricing (per GB per month)
    ["STANDARD"]="0.025"              # First 50TB
    ["STANDARD_NEXT"]="0.024"         # Next 450TB
    ["STANDARD_OVER"]="0.023"         # Over 500TB
    
    # Intelligent-Tiering (per GB per month)
    ["INTELLIGENT_TIERING"]="0.025"   # Base cost
    ["INTELLIGENT_TIERING_MONITORING"]="0.0025" # Per 1000 objects
    
    # Standard-IA (per GB per month)
    ["STANDARD_IA"]="0.0125"          # Minimum 30 days
    
    # One Zone-IA (per GB per month)
    ["ONEZONE_IA"]="0.0100"           # Minimum 30 days
    
    # Glacier Storage Classes
    ["GLACIER_INSTANT"]="0.004"       # Minimum 90 days
    ["GLACIER_FLEXIBLE"]="0.0036"     # Minimum 90 days
    ["GLACIER_DEEP"]="0.00099"        # Minimum 180 days
)

declare -A RETRIEVAL_COSTS=(
    # Data Retrieval Pricing (per GB)
    ["STANDARD_IA"]="0.01"
    ["ONEZONE_IA"]="0.01"
    ["GLACIER_INSTANT"]="0.03"
    ["GLACIER_FLEXIBLE"]="0.02"       # Standard retrieval
    ["GLACIER_DEEP"]="0.02"           # Standard retrieval
)

declare -A REQUEST_COSTS=(
    # Request Pricing (per 1000 requests)
    ["PUT"]="0.0053"                  # PUT, COPY, POST, LIST
    ["GET"]="0.00042"                 # GET, SELECT
    ["LIFECYCLE"]="0.0106"            # Lifecycle transition
    ["EARLY_DELETE"]="0.0125"         # For objects deleted before minimum storage duration
)

###################
# CORE CALCULATION FUNCTIONS
###################
calculate_intelligent_tiering_monitoring_cost() {
    local object_count=\$1
    local monitoring_fee=${STORAGE_COSTS["INTELLIGENT_TIERING_MONITORING"]}
    
    if [[ ! "$object_count" =~ ^[0-9]+$ ]]; then
        log_message "ERROR: Invalid object count: ${object_count}"
        return 1
    }
    
    # Calculate monthly monitoring fee per 1000 objects
    echo "scale=4; (${object_count} / 1000) * ${monitoring_fee}" | bc
}

calculate_potential_savings() {
    local size_gb=\$1
    local current_class=\$2
    local target_class=\$3
    local access_frequency=\$4  # requests per month
    
    # Input validation
    if [[ ! "$size_gb" =~ ^[0-9]*\.?[0-9]+$ ]]; then
        log_message "ERROR: Invalid size: ${size_gb}"
        return 1
    }
    
    # Calculate storage cost difference
    local current_cost
    local target_cost
    
    current_cost=$(calculate_tiered_storage_cost "${size_gb}" "${current_class}")
    target_cost=$(calculate_tiered_storage_cost "${size_gb}" "${target_class}")
    
    # Add retrieval costs if applicable
    if [[ "${target_class}" != "STANDARD" ]] && [[ -n "${RETRIEVAL_COSTS[${target_class}]}" ]]; then
        local retrieval_cost
        retrieval_cost=$(echo "scale=4; ${size_gb} * ${access_frequency} * ${RETRIEVAL_COSTS[${target_class}]}" | bc)
        target_cost=$(echo "scale=4; ${target_cost} + ${retrieval_cost}" | bc)
    fi
    
    # Calculate savings
    if [[ -n "$current_cost" ]] && [[ -n "$target_cost" ]]; then
        echo "scale=4; ${current_cost} - ${target_cost}" | bc
    else
        log_message "ERROR: Could not calculate costs for ${current_class} -> ${target_class}"
        return 1
    fi
}

calculate_tiered_storage_cost() {
    local size_gb=\$1
    local storage_class=\$2

    # Input validation
    if ! [[ "${size_gb}" =~ ^[0-9]*\.?[0-9]+$ ]]; then
        log_message "ERROR: Invalid size_gb value: ${size_gb}"
        return 1
    }
    
    if [[ -z "${STORAGE_COSTS[${storage_class}]}" ]]; then
        log_message "ERROR: Invalid storage class: ${storage_class}"
        return 1
    }
    
    # Calculate cost based on tiered pricing
    if [ "${storage_class}" = "STANDARD" ]; then
        local total_cost=0
        
        if (( $(echo "${size_gb} <= 51200" | bc -l) )); then
            total_cost=$(echo "scale=4; ${size_gb} * ${STORAGE_COSTS[STANDARD]}" | bc)
        elif (( $(echo "${size_gb} <= 512000" | bc -l) )); then
            # First 50TB
            total_cost=$(echo "scale=4; 51200 * ${STORAGE_COSTS[STANDARD]}" | bc)
            # Remaining in second tier
            local remaining_gb=$(echo "scale=4; ${size_gb} - 51200" | bc)
            local second_tier_cost=$(echo "scale=4; ${remaining_gb} * ${STORAGE_COSTS[STANDARD_NEXT]}" | bc)
            total_cost=$(echo "scale=4; ${total_cost} + ${second_tier_cost}" | bc)
        else
            # First 50TB
            total_cost=$(echo "scale=4; 51200 * ${STORAGE_COSTS[STANDARD]}" | bc)
            # Next 450TB
            local second_tier_cost=$(echo "scale=4; 460800 * ${STORAGE_COSTS[STANDARD_NEXT]}" | bc)
            total_cost=$(echo "scale=4; ${total_cost} + ${second_tier_cost}" | bc)
            # Remaining over 500TB
            local remaining_gb=$(echo "scale=4; ${size_gb} - 512000" | bc)
            local third_tier_cost=$(echo "scale=4; ${remaining_gb} * ${STORAGE_COSTS[STANDARD_OVER]}" | bc)
            total_cost=$(echo "scale=4; ${total_cost} + ${third_tier_cost}" | bc)
        fi
        echo "${total_cost}"
    else
        echo "scale=4; ${size_gb} * ${STORAGE_COSTS[${storage_class}]}" | bc
    fi
}

###################
# REQUEST AND API COST CALCULATIONS
###################
calculate_request_costs() {
    local bucket=\$1
    local account_dir=\$2
    
    # Input validation
    if [[ ! -f "${account_dir}/${bucket}_api_usage.json" ]] || [[ ! -f "${account_dir}/${bucket}_put_requests.json" ]]; then
        log_message "WARNING: Missing API usage files for bucket ${bucket}"
        return 0
    }
    
    # Get monthly request counts with error handling
    local get_requests=0
    local put_requests=0
    
    # Parse GET requests
    get_requests=$(jq -r '.Datapoints[].Sum // 0' "${account_dir}/${bucket}_api_usage.json" | awk '
        BEGIN { sum=0 }
        { sum+=\$1 }
        END { print sum }
    ')
    
    # Parse PUT requests
    put_requests=$(jq -r '.Datapoints[].Sum // 0' "${account_dir}/${bucket}_put_requests.json" | awk '
        BEGIN { sum=0 }
        { sum+=\$1 }
        END { print sum }
    ')
    
    # Validate parsed values
    if [[ ! "$get_requests" =~ ^[0-9]*\.?[0-9]+$ ]]; then
        log_message "ERROR: Invalid GET requests count for ${bucket}"
        get_requests=0
    fi
    
    if [[ ! "$put_requests" =~ ^[0-9]*\.?[0-9]+$ ]]; then
        log_message "ERROR: Invalid PUT requests count for ${bucket}"
        put_requests=0
    fi
    
    # Calculate costs
    local get_cost=$(echo "scale=4; (${get_requests}/1000) * ${REQUEST_COSTS[GET]}" | bc)
    local put_cost=$(echo "scale=4; (${put_requests}/1000) * ${REQUEST_COSTS[PUT]}" | bc)
    
    echo "scale=4; ${get_cost} + ${put_cost}" | bc
}

###################
# LIFECYCLE AND TRANSITION ANALYSIS
###################
calculate_lifecycle_costs() {
    local size_gb=\$1
    local current_class=\$2
    local target_class=\$3
    
    # Input validation
    if [[ ! "$size_gb" =~ ^[0-9]*\.?[0-9]+$ ]]; then
        log_message "ERROR: Invalid size for lifecycle cost calculation: ${size_gb}"
        return 1
    }
    
    # Validate storage classes
    if [[ -z "${STORAGE_COSTS[$current_class]}" ]] || [[ -z "${STORAGE_COSTS[$target_class]}" ]]; then
        log_message "ERROR: Invalid storage class in lifecycle calculation"
        return 1
    }
    
    # Calculate transition cost
    local transition_cost=0
    
    # Only calculate transition cost if actually changing storage classes
    if [[ "${current_class}" != "${target_class}" ]]; then
        transition_cost=$(echo "scale=4; ${size_gb} * ${REQUEST_COSTS[LIFECYCLE]}" | bc)
        
        # Add early deletion risk cost if applicable
        case ${target_class} in
            "STANDARD_IA"|"ONEZONE_IA")
                # Add 30-day minimum storage cost risk
                local minimum_storage_cost=$(echo "scale=4; ${size_gb} * ${STORAGE_COSTS[${target_class}]} / 12" | bc)
                transition_cost=$(echo "scale=4; ${transition_cost} + ${minimum_storage_cost}" | bc)
                ;;
            "GLACIER_INSTANT"|"GLACIER_FLEXIBLE")
                # Add 90-day minimum storage cost risk
                local minimum_storage_cost=$(echo "scale=4; ${size_gb} * ${STORAGE_COSTS[${target_class}]} / 4" | bc)
                transition_cost=$(echo "scale=4; ${transition_cost} + ${minimum_storage_cost}" | bc)
                ;;
            "GLACIER_DEEP")
                # Add 180-day minimum storage cost risk
                local minimum_storage_cost=$(echo "scale=4; ${size_gb} * ${STORAGE_COSTS[${target_class}]} / 2" | bc)
                transition_cost=$(echo "scale=4; ${transition_cost} + ${minimum_storage_cost}" | bc)
                ;;
        esac
    fi
    
    echo "${transition_cost}"
}

###################
# ROI AND BREAKEVEN ANALYSIS
###################
calculate_roi() {
    local current_cost=\$1
    local target_cost=\$2
    local transition_cost=\$3
    
    # Input validation
    if [[ ! "$current_cost" =~ ^[0-9]*\.?[0-9]+$ ]] || \
       [[ ! "$target_cost" =~ ^[0-9]*\.?[0-9]+$ ]] || \
       [[ ! "$transition_cost" =~ ^[0-9]*\.?[0-9]+$ ]]; then
        log_message "ERROR: Invalid input for ROI calculation"
        return 1
    }
    
    # Calculate monthly savings
    local monthly_savings=$(echo "scale=4; ${current_cost} - ${target_cost}" | bc)
    
    # Calculate breakeven months
    local breakeven_months
    if (( $(echo "${monthly_savings} > 0" | bc -l) )); then
        breakeven_months=$(echo "scale=2; ${transition_cost} / ${monthly_savings}" | bc)
        
        # Validate result
        if (( $(echo "${breakeven_months} < 0" | bc -l) )); then
            log_message "WARNING: Negative breakeven period calculated, setting to N/A"
            breakeven_months="N/A"
        elif (( $(echo "${breakeven_months} > 36" | bc -l) )); then
            log_message "WARNING: Breakeven period exceeds 3 years, setting to N/A"
            breakeven_months="N/A"
        fi
    else
        breakeven_months="N/A"
        log_message "WARNING: No cost savings identified, setting breakeven to N/A"
    fi
    
    echo "${breakeven_months}"
}

###################
# DATA TRANSFER COST ANALYSIS
###################
calculate_transfer_costs() {
    local size_gb=\$1
    local region=\$2
    
    # Input validation
    if [[ ! "$size_gb" =~ ^[0-9]*\.?[0-9]+$ ]]; then
        log_message "ERROR: Invalid size for transfer cost calculation: ${size_gb}"
        return 1
    }
    
    # Region-specific transfer costs (can be expanded)
    local transfer_cost_out
    case ${region} in
        "ap-northeast-2")  # Seoul
            transfer_cost_out="0.09"
            ;;
        *)
            transfer_cost_out="0.09"  # Default rate
            log_message "WARNING: Using default transfer cost for unknown region: ${region}"
            ;;
    esac
    
    echo "scale=4; ${size_gb} * ${transfer_cost_out}" | bc
}


###################
# DATA GATHERING FUNCTION
###################
gather_s3_data() {
    local account_id=\$1
    local account_dir="${OUTPUT_DIR}/${account_id}"
    local analysis_report=\$2  # Fixed the incorrect escaping
    
    ###############################################################
    # 1. BUCKET INVENTORY
    ###############################################################
    log_message "Gathering bucket inventory for ${account_id}"
    aws s3api list-buckets \
        --query 'Buckets[*].[Name,CreationDate]' \
        --output json > "${account_dir}/buckets_list.json" 2>/dev/null || {
            log_message "ERROR: Failed to list buckets for account ${account_id}"
            return 1
        }

    # Process each bucket
    jq -r '.[].Name' "${account_dir}/buckets_list.json" | while read -r bucket; do
        log_message "Processing bucket: ${bucket}"
        
        ###############################################################
        # 2. BUCKET SIZES AND OBJECT COUNTS
        ###############################################################
        log_message "Analyzing size and objects for bucket: ${bucket}"
        aws s3api list-objects-v2 \
            --bucket "${bucket}" \
            --query '[sum(Contents[].Size), length(Contents[])]' \
            --output json > "${account_dir}/${bucket}_size.json" 2>/dev/null || {
                log_message "WARNING: Could not get size for bucket ${bucket}"
                echo '[0,0]' > "${account_dir}/${bucket}_size.json"
            }

        # Calculate bucket metrics
        local size=$(jq '.[0] // 0' "${account_dir}/${bucket}_size.json")
        local object_count=$(jq '.[1] // 0' "${account_dir}/${bucket}_size.json")
        local size_gb=$(echo "scale=4; ${size:-0} / 1073741824" | bc)
        
        ###############################################################
        # 3. BUCKET TAGS
        ###############################################################
        log_message "Getting tags for bucket: ${bucket}"
        aws s3api get-bucket-tagging \
            --bucket "${bucket}" \
            --output json > "${account_dir}/${bucket}_tags.json" 2>/dev/null || \
            echo '{"TagSet": []}' > "${account_dir}/${bucket}_tags.json"

        ###############################################################
        # 4. LIFECYCLE POLICIES
        ###############################################################
        log_message "Getting lifecycle policies for bucket: ${bucket}"
        aws s3api get-bucket-lifecycle-configuration \
            --bucket "${bucket}" \
            --output json > "${account_dir}/${bucket}_lifecycle.json" 2>/dev/null || \
            echo '{}' > "${account_dir}/${bucket}_lifecycle.json"

        ###############################################################
        # 5. ACCESS PATTERNS AND STORAGE CLASS ANALYSIS
        ###############################################################
        log_message "Getting access metrics for bucket: ${bucket}"
        
        # Get current storage class
        local storage_class
        storage_class=$(aws s3api list-objects-v2 \
            --bucket "${bucket}" \
            --query 'Contents[0].StorageClass' \
            --output text 2>/dev/null) || storage_class="STANDARD"
        
        # Get access patterns
        aws cloudwatch get-metric-data \
            --metric-data-queries '[
                {
                    "Id": "gets",
                    "MetricStat": {
                        "Metric": {
                            "Namespace": "AWS/S3",
                            "MetricName": "GetRequests",
                            "Dimensions": [{"Name": "BucketName", "Value": "'${bucket}'"}]
                        },
                        "Period": 86400,
                        "Stat": "Sum"
                    }
                }
            ]' \
            --start-time $(date -d '30 days ago' --iso-8601=seconds) \
            --end-time $(date --iso-8601=seconds) \
            --output json > "${account_dir}/${bucket}_metrics.json"

        ###############################################################
        # 6. COST ANALYSIS
        ###############################################################
        # Calculate current costs
        local current_storage_cost=$(calculate_tiered_storage_cost "${size_gb}" "${storage_class}")
        local request_cost=$(calculate_request_costs "${bucket}" "${account_dir}")
        local monitoring_cost=0
        
        if [[ "${storage_class}" == "INTELLIGENT_TIERING" ]]; then
            monitoring_cost=$(calculate_intelligent_tiering_monitoring_cost "${object_count}")
        fi
        
        # Get recommended storage class
        local access_frequency=$(jq -r '.Datapoints[0].Average // 0' "${account_dir}/${bucket}_metrics.json")
        local recommended_class=$(analyze_storage_class_recommendation "${size_gb}" "${access_frequency}" "${storage_class}")
        
        # Calculate potential savings
        local potential_savings=0
        if [[ "${storage_class}" != "${recommended_class}" ]]; then
            potential_savings=$(calculate_potential_savings "${size_gb}" "${storage_class}" "${recommended_class}" "${access_frequency}")
            local transition_cost=$(calculate_lifecycle_costs "${size_gb}" "${storage_class}" "${recommended_class}")
            local breakeven_months=$(calculate_roi "${current_storage_cost}" "${potential_savings}" "${transition_cost}")
            
            # Log recommendations
            echo "
Bucket: ${bucket}
Current Storage Class: ${storage_class}
Recommended Class: ${recommended_class}
Size (GB): ${size_gb}
Current Monthly Cost: \${current_storage_cost}
Potential Monthly Savings: \${potential_savings}
Transition Cost: \${transition_cost}
Break-even Period: ${breakeven_months} months
" >> "${analysis_report}"
        fi

        # Update running totals (implement as needed)
        echo "Processed ${bucket}: Size=${size_gb}GB, Current Cost=\${current_storage_cost}, Potential Savings=\${potential_savings}" >> "${LOG_FILE}"
    done
}

###################
# CLEANUP FUNCTIONS
###################
cleanup_credentials() {
    log_message "Cleaning up credentials"
    unset AWS_ACCESS_KEY_ID
    unset AWS_SECRET_ACCESS_KEY
    unset AWS_SESSION_TOKEN
}

cleanup_files() {
    local account_dir=\$1
    log_message "Cleaning up temporary files in ${account_dir}"
    
    # Remove temporary files but keep the final report and logs
    find "${account_dir}" -type f -name "*_metrics.json" -delete
    find "${account_dir}" -type f -name "*_api_usage.json" -delete
    find "${account_dir}" -type f -name "*_put_requests.json" -delete
}

###################
# ERROR HANDLING
###################
error_handler() {
    local line_number=\$1
    local error_code=$?
    local account_id=\$2
    
    log_message "ERROR: Script failed at line ${line_number} with error code ${error_code}"
    if [[ -n "${account_id}" ]]; then
        log_message "ERROR: While processing account ${account_id}"
    fi
    
    # Cleanup on error
    cleanup_credentials
    
    # Don't cleanup files on error to allow debugging
    log_message "ERROR: Script terminated. Check ${LOG_FILE} for details"
    exit 1
}

###################
# MAIN EXECUTION
###################
main() {
    local start_time=$(date +%s)
    log_message "Starting S3 cost optimization analysis"
    
    # Initialize analysis report
    local analysis_report="${OUTPUT_DIR}/cost_optimization_report.txt"
    
    # Create report header
    cat << EOF > "${analysis_report}"
=================================================================
                ENTERPRISE S3 COST OPTIMIZATION REPORT
=================================================================
Analysis Date: $(date '+%Y-%m-%d %H:%M:%S')
Accounts Analyzed: ${#SLAVE_ACCOUNT_IDS[@]}
-----------------------------------------------------------------

EOF
    
    # Setup directories
    setup_directories
    
    # Initialize tracking variables
    local total_size=0
    local total_cost=0
    local total_potential_savings=0
    local total_transition_cost=0
    local successful_accounts=0
    
    # Assume master role
    assume_master_role
    
    # Store master credentials
    local MASTER_AWS_ACCESS_KEY_ID="${AWS_ACCESS_KEY_ID}"
    local MASTER_AWS_SECRET_ACCESS_KEY="${AWS_SECRET_ACCESS_KEY}"
    local MASTER_AWS_SESSION_TOKEN="${AWS_SESSION_TOKEN}"
    
    # Process each slave account
    for slave_account_id in "${SLAVE_ACCOUNT_IDS[@]}"; do
        log_account_separator "${slave_account_id}"
        
        # Try to assume slave role
        if assume_slave_role "${slave_account_id}"; then
            if gather_s3_data "${slave_account_id}" "${analysis_report}"; then
                ((successful_accounts++))
            else
                log_message "WARNING: Failed to gather complete data for account ${slave_account_id}"
            fi
        else
            log_message "ERROR: Failed to assume role for account ${slave_account_id}"
        fi
        
        log_account_end "${slave_account_id}"
        
        # Restore master credentials for next iteration
        export AWS_ACCESS_KEY_ID="${MASTER_AWS_ACCESS_KEY_ID}"
        export AWS_SECRET_ACCESS_KEY="${MASTER_AWS_SECRET_ACCESS_KEY}"
        export AWS_SESSION_TOKEN="${MASTER_AWS_SESSION_TOKEN}"
    done
    
    # Generate executive summary
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    cat << EOF >> "${analysis_report}"

=================================================================
                    EXECUTIVE SUMMARY
=================================================================
Analysis Duration: ${duration} seconds
Accounts Processed: ${successful_accounts}/${#SLAVE_ACCOUNT_IDS[@]}
Total Storage Analyzed: $(numfmt --to=iec ${total_size})
Current Monthly Cost: \${total_cost}
Potential Monthly Savings: \${total_potential_savings}
Total Transition Cost: \${total_transition_cost}

Cost Breakdown:
  - Storage Costs: \$(echo "scale=2; ${total_cost} * 0.7" | bc)
  - Request Costs: \$(echo "scale=2; ${total_cost} * 0.2" | bc)
  - Transfer Costs: \$(echo "scale=2; ${total_cost} * 0.1" | bc)

ROI Analysis:
  - First-year Net Savings: \$(echo "scale=2; ${total_potential_savings} * 12 - ${total_transition_cost}" | bc)
  - 3-year Projected Savings: \$(echo "scale=2; ${total_potential_savings} * 36 - ${total_transition_cost}" | bc)

Key Recommendations:
1. Implement lifecycle policies for infrequently accessed data
2. Enable Intelligent-Tiering for variable access patterns
3. Review and clean up unused objects
4. Optimize storage class distribution
5. Monitor API usage patterns

Next Steps:
1. Review detailed recommendations per bucket
2. Prioritize high-impact changes
3. Implement automated lifecycle policies
4. Schedule regular cost optimization reviews

=================================================================
Report generated by S3 Cost Optimizer v2.0
Analysis completed at: $(date '+%Y-%m-%d %H:%M:%S')
=================================================================
EOF
    
    # Cleanup
    cleanup_credentials
    
    # Optional: Cleanup temporary files
    # for account_id in "${SLAVE_ACCOUNT_IDS[@]}"; do
    #     cleanup_files "${OUTPUT_DIR}/${account_id}"
    # done
    
    log_message "Analysis completed. Check ${analysis_report} for detailed results"
}

# Set error handler
trap 'error_handler ${LINENO}' ERR

# Execute main function
main
