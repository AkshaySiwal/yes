def check_vpc_flow_logs_in_s3(slave_session):
    """
    Find all S3 buckets that store VPC Flow Logs
    Returns dictionary in format:
    {s3_bucket: {
        'vpc_id': vpc_id,
        'region': vpc_region,
        'traffic_type': traffic_type,
        'log_destination_prefix': prefix
    }}
    """
    try:
        result = {}

        # Initialize EC2 client
        ec2_client = slave_session.client('ec2')

        # Get all regions
        regions = [region['RegionName'] for region in ec2_client.describe_regions()['Regions']]

        for region in regions:
            try:
                # Create regional EC2 client
                regional_ec2 = slave_session.client('ec2', region_name=region)

                # Get flow logs in the region with S3 destination filter
                flow_logs = regional_ec2.describe_flow_logs(
                    Filters=[
                        {
                            'Name': 'log-destination-type',
                            'Values': ['s3']
                        }
                    ]
                )['FlowLogs']

                for flow_log in flow_logs:
                    log_destination = flow_log['LogDestination']
                    if log_destination.startswith('arn:aws:s3:::'):
                        bucket_name = log_destination.split(':::')[1].split('/')[0]
                        prefix = '/'.join(log_destination.split(':::')[1].split('/')[1:]) if '/' in log_destination else ''
                        vpc_id = flow_log.get('ResourceId', '') if flow_log.get('ResourceId', '').startswith('vpc-') else ''

                        if vpc_id:  # Only process if it's a VPC flow log
                            key = f"{bucket_name}_{vpc_id}"  # Create unique key for each bucket-vpc combination
                            result[key] = {
                                's3': bucket_name,
                                'vpc_id': vpc_id,
                                'region': region,
                                'traffic_type': flow_log.get('TrafficType', ''),
                                'log_destination_prefix': prefix
                            }

            except Exception as e:
                print(f"Error processing region {region}: {str(e)}")
                continue

        # Restructure the result to match desired format
        final_result = {}
        for item in result.values():
            bucket = item['s3']
            if bucket not in final_result:
                final_result[bucket] = {}

            vpc_id = item['vpc_id']
            final_result[bucket][vpc_id] = {
                'vpc_id': vpc_id,
                'region': item['region'],
                'traffic_type': item['traffic_type'],
                'log_destination_prefix': item['log_destination_prefix']
            }

        return final_result

    except Exception as e:
        print(f"Error checking VPC Flow Logs in S3: {str(e)}")
        return {}



def check_vpc_flow_logs_in_s3(slave_session):
    """
    Find all S3 buckets that store VPC Flow Logs
    Returns dictionary in format:
    {s3_bucket: {
        'vpc_id': vpc_id,
        'region': vpc_region,
        'traffic_type': traffic_type,
        'log_destination_prefix': prefix
    }}
    """
    try:
        result = {}

        # Initialize EC2 client
        ec2_client = slave_session.client('ec2')

        # Get all regions
        regions = [region['RegionName'] for region in ec2_client.describe_regions()['Regions']]

        for region in regions:
            try:
                # Create regional EC2 client
                regional_ec2 = slave_session.client('ec2', region_name=region)

                # Get flow logs in the region with S3 destination filter
                flow_logs = regional_ec2.describe_flow_logs(
                    Filters=[
                        {
                            'Name': 'log-destination-type',
                            'Values': ['s3']
                        }
                    ]
                )['FlowLogs']

                for flow_log in flow_logs:
                    log_destination = flow_log['LogDestination']
                    if log_destination.startswith('arn:aws:s3:::'):
                        bucket_name = log_destination.split(':::')[1].split('/')[0]
                        prefix = '/'.join(log_destination.split(':::')[1].split('/')[1:]) if '/' in log_destination else ''
                        vpc_id = flow_log.get('ResourceId', '') if flow_log.get('ResourceId', '').startswith('vpc-') else ''

                        if vpc_id:  # Only process if it's a VPC flow log
                            key = f"{bucket_name}_{vpc_id}"  # Create unique key for each bucket-vpc combination
                            result[key] = {
                                's3': bucket_name,
                                'vpc_id': vpc_id,
                                'region': region,
                                'traffic_type': flow_log.get('TrafficType', ''),
                                'log_destination_prefix': prefix
                            }

            except Exception as e:
                print(f"Error processing region {region}: {str(e)}")
                continue

        # Restructure the result to match desired format
        final_result = {}
        for item in result.values():
            bucket = item['s3']
            if bucket not in final_result:
                final_result[bucket] = {}

            vpc_id = item['vpc_id']
            final_result[bucket][vpc_id] = {
                'vpc_id': vpc_id,
                'region': item['region'],
                'traffic_type': item['traffic_type'],
                'log_destination_prefix': item['log_destination_prefix']
            }

        return final_result

    except Exception as e:
        print(f"Error checking VPC Flow Logs in S3: {str(e)}")
        return {}
